{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "from dateutil import parser\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title_1_site_1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to my database\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "client = MongoClient(port=12345) #Tunnel to my MongoDB client\n",
    "\n",
    "db = client.tech_news_data\n",
    "arts = db.articles\n",
    "arts.create_index([(\"title\", pymongo.ASCENDING), (\"site\", pymongo.ASCENDING)],unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TechCrunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with page 1\n",
      "done with page 2\n",
      "done with page 3\n",
      "done with page 4\n",
      "done with page 5\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get links from TechCrunch\n",
    "time_delay = 0.1\n",
    "nPages = 5\n",
    "start_page = 1\n",
    "\n",
    "url = \"https://techcrunch.com/page/\"\n",
    "\n",
    "#scrape feed pages for article links\n",
    "links = []\n",
    "for i in range(start_page,start_page+nPages):\n",
    "    response = requests.get(url + str(i))\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    data = soup.find('div',{\"role\": \"main\"}).find_all('h2', {'class': 'post-title'})\n",
    "    time.sleep(time_delay)\n",
    "    links.extend([data.find('a',href=True)['href'] for data in data])\n",
    "    print('done with page', i)\n",
    "    \n",
    "\n",
    "#ditch links we don't care about\n",
    "links = [l for l in links if '/video/' not in re.findall('/\\w*/', l)]\n",
    "links = [l for l in links if '/gallery/' not in re.findall('/\\w*/', l)]\n",
    "\n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 pages\n",
      "success adding 0\n",
      "success adding 1\n",
      "success adding 2\n",
      "success adding 3\n",
      "success adding 4\n",
      "success adding 5\n",
      "success adding 6\n",
      "success adding 7\n",
      "success adding 8\n",
      "success adding 9\n",
      "success adding 10\n",
      "success adding 11\n",
      "success adding 12\n",
      "success adding 13\n",
      "success adding 14\n",
      "success adding 15\n",
      "success adding 16\n",
      "success adding 17\n",
      "success adding 18\n",
      "success adding 19\n",
      "success adding 20\n",
      "success adding 21\n",
      "success adding 22\n",
      "success adding 23\n",
      "success adding 24\n",
      "success adding 25\n",
      "success adding 26\n",
      "success adding 27\n",
      "success adding 28\n",
      "success adding 29\n",
      "success adding 30\n",
      "success adding 31\n",
      "success adding 32\n",
      "success adding 33\n",
      "success adding 34\n",
      "success adding 35\n",
      "success adding 36\n",
      "success adding 37\n",
      "success adding 38\n",
      "success adding 39\n",
      "success adding 40\n",
      "success adding 41\n",
      "success adding 42\n",
      "success adding 43\n",
      "success adding 44\n",
      "success adding 45\n",
      "success adding 46\n",
      "success adding 47\n",
      "success adding 48\n",
      "success adding 49\n",
      "duplicate entry on \"Roku cracks down on private channels\"\n",
      "ending scrape\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape data from TechCrunch (most recent first); stop once we've stopped seeing new articles\n",
    "time_delay = 0.1\n",
    "\n",
    "site = 'TechCrunch'\n",
    "print(len(links), 'pages')\n",
    "for i, url in enumerate(links):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    title = soup.find('h1', {'class': 'alpha tweet-title'}).text\n",
    "    title = title.replace('\\xa0', '')\n",
    "    try:\n",
    "        author = soup.find('a', {'rel': 'author'}).text\n",
    "    except:\n",
    "        author = None\n",
    "    date = parser.parse(soup.find('div', {'class': 'byline'}).find('time', datetime=True)['datetime']).replace(hour=0,minute=0, second=0)\n",
    "    regex = re.compile('.*article-entry text.*')\n",
    "    body = ' '.join([i.text for i in soup.find('div', class_=regex).find_all('p')])\n",
    "    body = body.replace('\\xa0', '')\n",
    "    \n",
    "    tags = [a.find('a').text for a in soup.find_all('div', {'class': 'tag-item'})]\n",
    "    observation = {'title': title, 'author': author, 'date': date, 'body': body, 'site': site, 'tags': tags}\n",
    "    try:\n",
    "        arts.insert_one(observation)\n",
    "        print('success adding', i)\n",
    "    except DuplicateKeyError:\n",
    "        print('duplicate entry on', '\"'+ title + '\"')\n",
    "        print('ending scrape')\n",
    "        break\n",
    "    time.sleep(time_delay)\n",
    "    \n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArsTechnica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with page 1\n",
      "done with page 2\n",
      "done with page 3\n",
      "done with page 4\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get links from arstechnica\n",
    "time_delay = 0.5\n",
    "nPages = 4\n",
    "start_page = 1\n",
    "\n",
    "url = \"https://arstechnica.com/page/\"\n",
    "\n",
    "links = []\n",
    "for i in range(start_page,start_page+nPages):\n",
    "    response = requests.get(url + str(i))\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    data = soup.find_all('article')\n",
    "    time.sleep(time_delay)\n",
    "    links.extend([data.find('a',href=True)['href'] for data in data])\n",
    "    print('done with page', i)\n",
    "    \n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 pages\n",
      "error: duplicate entry on How the tech sector can legally justify breaking ties to extremists\n",
      "ending scrape\n",
      "error: duplicate entry on Review: System76’s Galago Pro solves “just works” Linux’s Goldilocks problem\n",
      "ending scrape\n",
      "success adding 2\n",
      "success adding 3\n",
      "success adding 4\n",
      "success adding 5\n",
      "success adding 6\n",
      "success adding 7\n",
      "success adding 8\n",
      "success adding 9\n",
      "success adding 10\n",
      "success adding 11\n",
      "success adding 12\n",
      "success adding 13\n",
      "success adding 14\n",
      "success adding 15\n",
      "success adding 16\n",
      "success adding 17\n",
      "error: duplicate entry on Racist Daily Stormer goes down again as CloudFlare drops support\n",
      "ending scrape\n",
      "success adding 19\n",
      "success adding 20\n",
      "success adding 21\n",
      "error: duplicate entry on Nokia 8: An all-aluminum flagship with same-day Android security updates\n",
      "ending scrape\n",
      "error: duplicate entry on FCC giving special help to right-wing TV news company, Democrats allege\n",
      "ending scrape\n",
      "error: duplicate entry on The origin of complex life on Earth just got a little less mysterious\n",
      "ending scrape\n",
      "error: duplicate entry on Bay Area: Join us tonight, 8/16, to talk about Silicon Valley and the inequality crisis\n",
      "ending scrape\n",
      "error: duplicate entry on Bank-fraud malware not detected by any AV hosted in Chrome Web Store. Twice\n",
      "ending scrape\n",
      "error: duplicate entry on Ukraine malware author turns witness in Russian DNC hacking investigation\n",
      "ending scrape\n",
      "error: duplicate entry on Lawyers clash over an imaged hard drive as Waymo v. Uber hurtles toward trial\n",
      "ending scrape\n",
      "error: duplicate entry on Deadly drug-resistant fungus sparks outbreaks in UK—and it’s stalking US\n",
      "ending scrape\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4b22c75b33f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ending scrape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_delay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DONE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#scrape data from ArsTechnica links; stop once we've stopped adding new articles\n",
    "time_delay = 0.5\n",
    "\n",
    "site ='ArsTechnica'\n",
    "print(len(links), 'pages')\n",
    "for i, url in enumerate(links):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    title = soup.find('h1', {'itemprop': 'headline'}).text\n",
    "    title = title.replace('\\xa0', '')\n",
    "    author = soup.find('p', {'class':'byline'}).find('span', {'itemprop': 'name'}).text\n",
    "    date = parser.parse(soup.find('p', {'class':'byline'}).find('time').text).replace(hour=0,minute=0, second=0)\n",
    "    body = ' '.join([a.text for a in soup.find('div', {'itemprop': 'articleBody'}).find_all('p')])\n",
    "    body = body.replace('\\xa0', '')\n",
    "    \n",
    "    tags = [i.split('/')[1] for i in re.findall('com/[a-zA-Z]+/', links[0])]\n",
    "    \n",
    "    observation = {'title': title, 'author': author, 'date': date, 'body': body, 'site': site, 'tags': tags}\n",
    "    try:\n",
    "        arts.insert_one(observation)\n",
    "        print('success adding', i)\n",
    "    except DuplicateKeyError:\n",
    "        print('error: duplicate entry on', title)\n",
    "        print('ending scrape')\n",
    "        break\n",
    "    time.sleep(time_delay)\n",
    "    \n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engadget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with page 1\n",
      "done with page 2\n",
      "done with page 3\n",
      "done with page 4\n",
      "done with page 5\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get links from arstechnica\n",
    "time_delay = 0.5\n",
    "nPages = 5\n",
    "start_page = 1\n",
    "\n",
    "url = \"https://www.engadget.com/all/page/\"\n",
    "\n",
    "links = []\n",
    "for i in range(start_page,start_page+nPages):\n",
    "    response = requests.get(url + str(i))\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    data = [x['href'] for x in soup.find('div', class_=\"pb-40@m- \").find_all('a',href=True, class_=\"o-hit__link\")]\n",
    "    time.sleep(time_delay)\n",
    "    links.extend(data)\n",
    "    print('done with page', i)\n",
    "    \n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 pages\n",
      "success adding 0\n",
      "success adding 1\n",
      "success adding 2\n",
      "success adding 3\n",
      "success adding 4\n",
      "success adding 5\n",
      "success adding 6\n",
      "success adding 7\n",
      "success adding 8\n",
      "success adding 9\n",
      "success adding 10\n",
      "success adding 11\n",
      "success adding 12\n"
     ]
    }
   ],
   "source": [
    "#scrape data from ArsTechnica links; stop once we've stopped adding new articles\n",
    "time_delay = 0.5\n",
    "\n",
    "site ='Engadget'\n",
    "print(len(links), 'pages')\n",
    "for i, url in enumerate(links[10:]):\n",
    "    url = 'https://www.engadget.com' + url\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    try:\n",
    "        title = soup.find('h1', {'class': 't-h4@m- t-h1-b@tp t-h1@tl+ mt-20 mt-15@tp mt-0@m-'}).text\n",
    "        title = title.replace('\\xa0', '')\n",
    "        author = soup.find('div', {'class':\"t-meta-small@s t-meta@m+\"}).find('a').text\n",
    "\n",
    "        body = ' '.join([a.text for a in soup.find('div', class_=\"flush-top flush-bottom\").find_all('p')])\n",
    "        body = body.replace('\\xa0', '')\n",
    "        #print(body)\n",
    "\n",
    "        tags = [soup.find('div', class_=\"grid@tl+__cell col-12-of-12@tp col-5-of-12@tl+ vm\").find('div', class_=\"th-meta\").find('a')['href']]\n",
    "        tags = [i.split('/')[-2] for i in tags]\n",
    "\n",
    "        date_string = re.findall('[0-9]+', url)\n",
    "        date = '/'.join(date_string[:3])\n",
    "        date = parser.parse(date)\n",
    "\n",
    "        observation = {'title': title, 'author': author, 'date': date, 'body': body, 'site': site, 'tags': tags}\n",
    "        try:\n",
    "            arts.insert_one(observation)\n",
    "            print('success adding', i)\n",
    "        except DuplicateKeyError:\n",
    "            print('error: duplicate entry on', title)\n",
    "            print('ending scrape')\n",
    "            break\n",
    "    except:\n",
    "        print('error on article', i)\n",
    "    time.sleep(time_delay)\n",
    "    \n",
    "print('DONE')\n",
    "subprocess.call([\"C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", \"[console]::beep(500,1000)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
